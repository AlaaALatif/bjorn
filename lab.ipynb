{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bjorn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "python3 bjorn.py --not-dry-run --include-bams --out-dir /home/gk/southpark/2020-11-21_release --sample-sheet /home/gk/code/hCoV19/release_summary_csv/2020-11-20_seq_summary.csv --cpus 25 --analysis-folder /home/gk/analysis/ --output-metadata /home/gk/analysis/hcov-19-genomics/metadata.csv\r\n"
     ]
    }
   ],
   "source": [
    "!cat run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether or not to include bam files in the release\n",
    "include_bams = False\n",
    "# these are the columns to include in the metadata.csv output\n",
    "meta_columns = ['sample_id', 'Virus name', 'Submitting lab', 'Location', 'Collection date', 'AVG_DEPTH', 'COVERAGE']\n",
    "# path to reference sequence (used later for MSA and tree construction)\n",
    "# this is the directory where results get saved\n",
    "out_dir = Path('/home/al/data/bjorn_test')\n",
    "# number of cores to use\n",
    "num_cpus = 4\n",
    "# file path to samples sheet (make sure it is the most recent)\n",
    "sample_sheet_fpath = Path('/home/al/data/COVID_sequencing_summary - GISAID.csv')\n",
    "# path to analysis results\n",
    "analysis_fpath = Path('/home/gk/analysis/')\n",
    "# file path to metadata of samples that have already been released\n",
    "released_samples_fpath = Path('/home/gk/analysis/hcov-19-genomics/metadata.csv')\n",
    "# Whether run is dry\n",
    "dry_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dry run: True.\n",
      "Include BAMS: False.\n",
      "Reading release summary file from /home/al/data/COVID_sequencing_summary - GISAID.csv.\n",
      "Reading repository metadata from /home/gk/analysis/hcov-19-genomics/metadata.csv.\n",
      "Searching analysis folder /home/gk/analysis/.\n",
      "\n",
      "Preparing 0 sammples for release\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Test\n",
    "# out_dir = \"/home/gk/southpark/2020-11-21_release\"\n",
    "# sample_sheet_fpath = \"/home/gk/code/hCoV19/release_summary_csv/2020-11-20_seq_summary.csv\"\n",
    "# analysis_fpath = \"/home/gk/analysis/\"\n",
    "# released_samples_fpath = \"/home/gk/analysis/hcov-19-genomics/metadata.csv\"\n",
    "# dry_run = True\n",
    "\n",
    "print(f\"\"\"\n",
    "Dry run: {dry_run}.\n",
    "Include BAMS: {include_bams}.\n",
    "Reading release summary file from {sample_sheet_fpath}.\n",
    "Reading repository metadata from {released_samples_fpath}.\n",
    "Searching analysis folder {analysis_fpath}.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Collecting Sequence Data\n",
    "\n",
    "\n",
    "# grab all filepaths for bam data\n",
    "bam_filepaths = glob.glob(\"{}/**/merged_aligned_bams/illumina/*.bam\".format(analysis_fpath))\n",
    "bam_filepaths = [Path(fp) for fp in bam_filepaths]\n",
    "# consolidate sample ID format\n",
    "bam_ids = get_ids(bam_filepaths)\n",
    "# Turn into dataframe\n",
    "bam_data = list(zip(*[bam_ids, bam_filepaths]))\n",
    "bam_df = pd.DataFrame(data=bam_data, columns=['sample_id', 'PATH'])\n",
    "# grab all paths to consensus sequences\n",
    "consensus_filepaths = glob.glob(\"{}/**/consensus_sequences/illumina/*.fa\".format(analysis_fpath))\n",
    "consensus_filepaths = [Path(fp) for fp in consensus_filepaths]\n",
    "# consolidate sample ID format\n",
    "consensus_ids = get_ids(consensus_filepaths)\n",
    "# Turn into dataframe\n",
    "consensus_data = list(zip(*[consensus_ids, consensus_filepaths]))\n",
    "consensus_df = pd.DataFrame(data=consensus_data, columns=['sample_id', 'PATH'])\n",
    "# clean up cns and bam (remove duplicate IDs)\n",
    "bam_df.drop_duplicates(subset=['sample_id'], keep='last', inplace=True)\n",
    "consensus_df.drop_duplicates(subset=['sample_id'], keep='last', inplace=True)\n",
    "# include only SEARCH samples\n",
    "consensus_df = consensus_df[(consensus_df['sample_id'].str.contains('SEARCH'))]\n",
    "# merge consensus and bam filepaths for each sample ID\n",
    "analysis_df = pd.merge(consensus_df, bam_df, on='sample_id', how='left')\n",
    "# exclude any samples that do not have BAM data\n",
    "analysis_df = analysis_df[~analysis_df['PATH_y'].isna()]\n",
    "# load sample sheet data (GISAID) - make sure to download most recent one\n",
    "seqsum = pd.read_csv(sample_sheet_fpath)\n",
    "# clean up\n",
    "seqsum = seqsum[(~seqsum['SEARCH SampleID'].isna()) & (seqsum['SEARCH SampleID']!='#REF!')]\n",
    "# consolidate sample ID format\n",
    "seqsum.loc[:, 'sample_id'] = seqsum['SEARCH SampleID'].apply(process_id)\n",
    "seqsum.drop_duplicates(subset=['sample_id'], keep='last', inplace=True)\n",
    "seqsum = seqsum[seqsum['New sequences ready for release'] == 'Yes']\n",
    "# JOIN summary sheet with analysis meta data\n",
    "sequence_results = pd.merge(seqsum, analysis_df, on='sample_id', how='inner')\n",
    "print(\"Preparing {} samples for release\".format(sequence_results.shape[0]))\n",
    "# samples missing consensus or BAM sequence files\n",
    "num_samples_missing_bams = sequence_results[sequence_results['PATH_y'].isna()].shape[0]\n",
    "num_samples_missing_cons = sequence_results[sequence_results['PATH_x'].isna()].shape[0]\n",
    "# ## Make sure to remove any samples that have already been uploaded to github (just an extra safety step)\n",
    "# load metadata.csv from github repo, then clean up\n",
    "meta_df = pd.read_csv(released_samples_fpath)\n",
    "meta_df = meta_df[meta_df['ID'].str.contains('SEARCH')]\n",
    "# consolidate sample ID format\n",
    "meta_df.loc[:, 'sample_id'] = meta_df['ID'].apply(process_id)\n",
    "# meta_df['sample_id']\n",
    "# get IDs of samples that have already been released\n",
    "released_seqs = meta_df['sample_id'].unique()\n",
    "# filter out released samples from all the samples we got\n",
    "final_result = sequence_results[~sequence_results['sample_id'].isin(released_seqs)]\n",
    "# Transfer files\n",
    "if not dry_run:\n",
    "    transfer_files(final_result, out_dir, include_bams=include_bams, ncpus=num_cpus)\n",
    "# ## Getting coverage information\n",
    "cov_filepaths = glob.glob(\"{}/**/trimmed_bams/illumina/reports/*.tsv\".format(analysis_fpath))\n",
    "# get_ipython().getoutput(\"find {analysis_fpath} -type f -path '*trimmed_bams/illumina/reports*' -name '*.tsv'\")\n",
    "cov_filepaths = [Path(fp) for fp in cov_filepaths]\n",
    "# read coverage data and clean it up\n",
    "cov_df = pd.concat((pd.read_csv(f, sep='\\t').assign(path=f) for f in cov_filepaths))\n",
    "cov_df.loc[:,'sample_id'] = cov_df['SAMPLE'].apply(process_coverage_sample_ids)\n",
    "cov_df.loc[:,'date'] = cov_df['path'].apply(lambda x: ''.join(x.split('/')[4].split('.')[:3]))\n",
    "cov_df = (cov_df.sort_values('date')\n",
    "          .drop_duplicates(subset=['sample_id'], keep='last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
